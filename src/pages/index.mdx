---
layout: ../layouts/Layout.astro
title: "Using Large Language Models to Generate JUnit Tests: An Empirical Study"
description: "Using Large Language Models to Generate JUnit Tests: An Empirical Study" 
favicon: /favicon.svg
thumbnail: /EASE2024.png
---

import { Image } from "astro:assets";

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import TwoColumns from "../components/TwoColumns.astro";
import Video from "../components/Video.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import PDF from "../components/PDF.astro";
import Figure from "../components/Figure.astro";
import LaTeX from "../components/LaTeX.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
export const components = {pre: CodeBlock}

import infographic from "../assets/ease-2024-infographic.png";

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Mohammed Latif Siddiq",
      url: "https://lsiddiqsunny.github.io/",
      institution: "University of Notre Dame",
    },
    {
      name: "Joanna C. S. Santos",
      url: "https://joannacss.github.io/",
      institution: "University of Notre Dame",
    },
    {
      name: "Ridwanul Hasan Tanvir",
      institution: "Pennsylvania State University",
    },
    {
      name: "Noshin Ulfat",
      institution: "IQVIA Inc.",
    },
    {
      name: "Fahmid Al Rifat",
      institution: "United International University",
    },
    {
      name: "Vinicius Carvalho Lopes",
      institution: "University of Notre Dame",
    },
  ]}
  conference="The 28th International Conference on Evaluation and Assessment in Software Engineering (EASE '24)"
  links={[
    {
      name: "Paper",
      url: "https://s2e-lab.github.io/preprints/ease24-preprint.pdf",
      icon: "fa-solid:file-pdf",
    },
    {
      name: "Code",
      url: "https://github.com/s2e-lab/LLM-Based-Test-Generation-Study",
      icon: "mdi:github",
    },
    {
      name: "arXiv",
      url: "https://arxiv.org/abs/2305.00418",
      icon: "academicons:arxiv",
    },
  ]}
  />


## Abstract

A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.


## Infographic

<Figure
    caption="Summary of the paper."
  >
    <Image src={infographic} alt="infographic" />
</Figure>


## BibTeX citation

```
@inproceedings{siddiq2024junit,
  author={Siddiq, Mohammed Latif and Santos, Joanna C. S. and Tanvir, Ridwanul Hasan and Ulfat, Noshin and Rifat, Fahmid Al and Lopes, Vinicius Carvalho},
  title={Using Large Language Models to Generate JUnit Tests: An Empirical Study}, 
  booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
  pages = {313â€“322},
  numpages = {10},
  keywords = {junit, large language models, test generation, test smells, unit testing},
  doi = {10.1145/3661167.3661216}
  location = {Salerno, Italy},
  series = {EASE '24}
}
```